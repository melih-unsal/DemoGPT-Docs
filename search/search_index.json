{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DemoGPT Welcome to the documentation for DemoGPT. This application uses OpenAI's GPT-3.5-turbo model to automate the process of creating Large Language Model (LLM) based applications. It refines and generates Python code, constructs LangChain pipelines, and offers an interactive Streamlit interface for users to work with. Core Functionalities of DemoGPT Automatic LangChain Pipelines : DemoGPT creates automatic LangChain pipelines that enable the effortless creation of LLM-based applications. It takes advantage of LangChain, a framework designed for developing applications powered by language models. Interactive Demo Generation : DemoGPT empowers developers to create engaging and interactive product demos. It generates applications that showcase key features, providing a hands-on experience with simulated interactions and real-time visualizations. Version Control and Management : DemoGPT simplifies version control, enabling seamless management and generation of multiple product versions. It tracks changes to ensure structured version management and allows developers to showcase different iterations and enhancements to stakeholders. (Note: This feature is planned for future development) Automated Pipeline Generation : DemoGPT automates pipeline generation, saving developers time and effort. It intelligently constructs data processing pipelines based on specific requirements, streamlining the development process and eliminating the need for manual construction. Modules in DemoGPT Model : The LogicModel and StreamlitModel classes interact with the OpenAI API and generate Python code, respectively. Explore how these two models work together to deliver the core functionalities of DemoGPT. Prompts : Learn how we use ChatPromptTemplate instances to guide the GPT-3.5-turbo model for code generation, testing, and refining in a structured manner. App : Discover the main Streamlit application that integrates all the components and serves as the user interface for our code generator. Please navigate through the links to learn more about each module.","title":"Home"},{"location":"#demogpt","text":"Welcome to the documentation for DemoGPT. This application uses OpenAI's GPT-3.5-turbo model to automate the process of creating Large Language Model (LLM) based applications. It refines and generates Python code, constructs LangChain pipelines, and offers an interactive Streamlit interface for users to work with.","title":"DemoGPT"},{"location":"#core-functionalities-of-demogpt","text":"Automatic LangChain Pipelines : DemoGPT creates automatic LangChain pipelines that enable the effortless creation of LLM-based applications. It takes advantage of LangChain, a framework designed for developing applications powered by language models. Interactive Demo Generation : DemoGPT empowers developers to create engaging and interactive product demos. It generates applications that showcase key features, providing a hands-on experience with simulated interactions and real-time visualizations. Version Control and Management : DemoGPT simplifies version control, enabling seamless management and generation of multiple product versions. It tracks changes to ensure structured version management and allows developers to showcase different iterations and enhancements to stakeholders. (Note: This feature is planned for future development) Automated Pipeline Generation : DemoGPT automates pipeline generation, saving developers time and effort. It intelligently constructs data processing pipelines based on specific requirements, streamlining the development process and eliminating the need for manual construction.","title":"Core Functionalities of  DemoGPT"},{"location":"#modules-in-demogpt","text":"Model : The LogicModel and StreamlitModel classes interact with the OpenAI API and generate Python code, respectively. Explore how these two models work together to deliver the core functionalities of DemoGPT. Prompts : Learn how we use ChatPromptTemplate instances to guide the GPT-3.5-turbo model for code generation, testing, and refining in a structured manner. App : Discover the main Streamlit application that integrates all the components and serves as the user interface for our code generator. Please navigate through the links to learn more about each module.","title":"Modules in  DemoGPT"},{"location":"app/","text":"Streamlit Application Documentation Introduction This Python Streamlit application uses LogicModel and StreamlitModel from the model module to generate and execute Python code based on user input. The user can input their idea, and the application will generate code, refine it, test it, and display the results. The Streamlit web app allows users to interact with the model in real time, which is particularly useful for demonstrating the capabilities of the models. Application Flow Importing Dependencies At the beginning of the application, all necessary modules such as streamlit , model , os , logging , webbrowser , and signal are imported. The logging level is set to DEBUG with the format 'levelname-message'. Loading Environment Variables The application tries to load environment variables using the dotenv module. If the module is not present, it logs an error message but continues to execute the application. Generate Response The function generate_response uses the LogicModel to generate responses for the given text. It's a generator function yielding the output of the LogicModel in each iteration. Streamlit Configuration The title of the Streamlit page is set using st.set_page_config . Input Fields Input fields for the OpenAI API Key, demo title, and demo idea are created using st.sidebar.text_input , st.text_input , and st.text_area respectively. The OpenAI API Key defaults to the value of the environment variable 'OPENAI_API_KEY'. Submission Form A form is created to handle the submission of user input. If the user submits the form, the application checks if a valid OpenAI API Key is entered. If not, it displays a warning message. If the input is valid, instances of LogicModel and StreamlitModel are created with the provided OpenAI API Key. Running the Model The application then enters a loop where it generates, refines, tests and executes code using the LogicModel . The progress of this process is displayed on a Streamlit progress bar. If the code execution is successful, it launches a new Streamlit application running the generated code and opens the new application in the web browser. In case the execution was not successful, the application refines the code and retries. If all attempts are unsuccessful, it reports a failure.","title":"App"},{"location":"app/#streamlit-application-documentation","text":"","title":"Streamlit Application Documentation"},{"location":"app/#introduction","text":"This Python Streamlit application uses LogicModel and StreamlitModel from the model module to generate and execute Python code based on user input. The user can input their idea, and the application will generate code, refine it, test it, and display the results. The Streamlit web app allows users to interact with the model in real time, which is particularly useful for demonstrating the capabilities of the models.","title":"Introduction"},{"location":"app/#application-flow","text":"","title":"Application Flow"},{"location":"app/#importing-dependencies","text":"At the beginning of the application, all necessary modules such as streamlit , model , os , logging , webbrowser , and signal are imported. The logging level is set to DEBUG with the format 'levelname-message'.","title":"Importing Dependencies"},{"location":"app/#loading-environment-variables","text":"The application tries to load environment variables using the dotenv module. If the module is not present, it logs an error message but continues to execute the application.","title":"Loading Environment Variables"},{"location":"app/#generate-response","text":"The function generate_response uses the LogicModel to generate responses for the given text. It's a generator function yielding the output of the LogicModel in each iteration.","title":"Generate Response"},{"location":"app/#streamlit-configuration","text":"The title of the Streamlit page is set using st.set_page_config .","title":"Streamlit Configuration"},{"location":"app/#input-fields","text":"Input fields for the OpenAI API Key, demo title, and demo idea are created using st.sidebar.text_input , st.text_input , and st.text_area respectively. The OpenAI API Key defaults to the value of the environment variable 'OPENAI_API_KEY'.","title":"Input Fields"},{"location":"app/#submission-form","text":"A form is created to handle the submission of user input. If the user submits the form, the application checks if a valid OpenAI API Key is entered. If not, it displays a warning message. If the input is valid, instances of LogicModel and StreamlitModel are created with the provided OpenAI API Key.","title":"Submission Form"},{"location":"app/#running-the-model","text":"The application then enters a loop where it generates, refines, tests and executes code using the LogicModel . The progress of this process is displayed on a Streamlit progress bar. If the code execution is successful, it launches a new Streamlit application running the generated code and opens the new application in the web browser. In case the execution was not successful, the application refines the code and retries. If all attempts are unsuccessful, it reports a failure.","title":"Running the Model"},{"location":"model/","text":"Models Overview This section provides an overview of the models used in DemoGPT. Currently, we are using two main models, LogicModel and StreamlitModel . These models interact with the OpenAI GPT-3.5-turbo to perform specific tasks and generate LangChain pipelines automatically. Introduction This Python module contains the implementation of three main classes - BaseModel , LogicModel , and StreamlitModel . The classes encapsulate a way to interact with OpenAI and the logic to refine and execute python code using a subprocess, while also managing temporary files. Classes BaseModel The BaseModel is a foundational class used to interact with OpenAI. Attributes: openai_api_key : The API key for OpenAI. llm : An instance of the ChatOpenAI class. Methods: __init__(self,openai_api_key) : A constructor that initializes the BaseModel with the provided OpenAI API key. refine_code(self,code) : A method that refines python code removing markdown code block syntax. LogicModel LogicModel is a class used to interact with OpenAI to obtain python code and subsequently refine, test, and execute it. Attributes: code_chain : An instance of LLMChain with the code_prompt . test_chain : An instance of LLMChain with the test_prompt . Other chain attributes for refining, fixing, and checking code. Methods: __init__(self,openai_api_key) : A constructor that initializes the LogicModel with the provided OpenAI API key. addDocuments(self) : This method reads text from a specified file and saves it to self.document . decode_results(self, results) : This method decodes the result of the subprocess command. run_python(self,code) : This method executes python code using a subprocess and decodes the result. __call__(self,topic,num_iterations=10) : This method uses the various chains to generate, refine, and test python code based on a given topic. StreamlitModel StreamlitModel is a class that uses OpenAI to create a Streamlit app, which is then run as a subprocess. Attributes: streamlit_code_chain : An instance of LLMChain with the streamlit_code_prompt . Methods: __init__(self,openai_api_key) : A constructor that initializes the StreamlitModel with the provided OpenAI API key. run_code(self,code) : This method writes the Streamlit code to a temporary file and runs it as a subprocess. __call__(self,topic, title, code, test_code,progress_func,success_func) : This method generates the Streamlit code, executes it, and returns the process ID.","title":"Model"},{"location":"model/#models-overview","text":"This section provides an overview of the models used in DemoGPT. Currently, we are using two main models, LogicModel and StreamlitModel . These models interact with the OpenAI GPT-3.5-turbo to perform specific tasks and generate LangChain pipelines automatically.","title":"Models Overview"},{"location":"model/#introduction","text":"This Python module contains the implementation of three main classes - BaseModel , LogicModel , and StreamlitModel . The classes encapsulate a way to interact with OpenAI and the logic to refine and execute python code using a subprocess, while also managing temporary files.","title":"Introduction"},{"location":"model/#classes","text":"","title":"Classes"},{"location":"model/#basemodel","text":"The BaseModel is a foundational class used to interact with OpenAI.","title":"BaseModel"},{"location":"model/#attributes","text":"openai_api_key : The API key for OpenAI. llm : An instance of the ChatOpenAI class.","title":"Attributes:"},{"location":"model/#methods","text":"__init__(self,openai_api_key) : A constructor that initializes the BaseModel with the provided OpenAI API key. refine_code(self,code) : A method that refines python code removing markdown code block syntax.","title":"Methods:"},{"location":"model/#logicmodel","text":"LogicModel is a class used to interact with OpenAI to obtain python code and subsequently refine, test, and execute it.","title":"LogicModel"},{"location":"model/#attributes_1","text":"code_chain : An instance of LLMChain with the code_prompt . test_chain : An instance of LLMChain with the test_prompt . Other chain attributes for refining, fixing, and checking code.","title":"Attributes:"},{"location":"model/#methods_1","text":"__init__(self,openai_api_key) : A constructor that initializes the LogicModel with the provided OpenAI API key. addDocuments(self) : This method reads text from a specified file and saves it to self.document . decode_results(self, results) : This method decodes the result of the subprocess command. run_python(self,code) : This method executes python code using a subprocess and decodes the result. __call__(self,topic,num_iterations=10) : This method uses the various chains to generate, refine, and test python code based on a given topic.","title":"Methods:"},{"location":"model/#streamlitmodel","text":"StreamlitModel is a class that uses OpenAI to create a Streamlit app, which is then run as a subprocess.","title":"StreamlitModel"},{"location":"model/#attributes_2","text":"streamlit_code_chain : An instance of LLMChain with the streamlit_code_prompt .","title":"Attributes:"},{"location":"model/#methods_2","text":"__init__(self,openai_api_key) : A constructor that initializes the StreamlitModel with the provided OpenAI API key. run_code(self,code) : This method writes the Streamlit code to a temporary file and runs it as a subprocess. __call__(self,topic, title, code, test_code,progress_func,success_func) : This method generates the Streamlit code, executes it, and returns the process ID.","title":"Methods:"},{"location":"prompts/","text":"Prompts Module Documentation The prompts.py file serves as a crucial component of the DemoGPT project. It contains structured prompts for directing OpenAI's GPT-3.5-turbo model in the process of code generation, testing, refinement, bug fixing, and Streamlit application generation. These are implemented through a series of ChatPromptTemplate instances. Prompt Templates code_prompt : Generates Python code to meet a certain goal. The model is instructed to produce Python code similar to the codes in a supplied document. test_prompt : Produces a function call for testing a previously generated Python function. It prompts the model to create sample input and call the function inside the given Python script. refine_chat_prompt : Improves a content piece based on the provided critics, utilizing a reference document. The model is asked to refine a code snippet considering the error provided. fix_chat_prompt : Identifies and fixes bugs in a Python code snippet based on the provided error message. check_chat_prompt : Verifies if a Python code snippet is functioning as expected by examining the output. If the model decides that the output is inappropriate, it refines the code; otherwise, it returns the original code. streamlit_code_prompt : Generates Streamlit application code from a given goal, logic code, and test code. The model is tasked with creating Streamlit code that achieves the same objective as the test code. Understanding Prompt Templates Each prompt template is assembled from system and/or human message prompts. These prompts originate from template strings that characterize the model's role and define the task it needs to complete. The ChatPromptTemplate.from_messages() function compiles these message prompts into a comprehensive chat prompt that instructs the model to produce the required output. This method accepts an array of system and human message prompts, with the sequence they appear in the array determining the conversation flow with the model. Usage These prompt templates are utilized throughout the DemoGPT project to guide the GPT-3.5-turbo model in generating, testing, refining, and fixing Python code, as well as creating Streamlit applications. They form the basic building blocks of the project's interaction with the language model.","title":"Prompts"},{"location":"prompts/#prompts-module-documentation","text":"The prompts.py file serves as a crucial component of the DemoGPT project. It contains structured prompts for directing OpenAI's GPT-3.5-turbo model in the process of code generation, testing, refinement, bug fixing, and Streamlit application generation. These are implemented through a series of ChatPromptTemplate instances.","title":"Prompts Module Documentation"},{"location":"prompts/#prompt-templates","text":"code_prompt : Generates Python code to meet a certain goal. The model is instructed to produce Python code similar to the codes in a supplied document. test_prompt : Produces a function call for testing a previously generated Python function. It prompts the model to create sample input and call the function inside the given Python script. refine_chat_prompt : Improves a content piece based on the provided critics, utilizing a reference document. The model is asked to refine a code snippet considering the error provided. fix_chat_prompt : Identifies and fixes bugs in a Python code snippet based on the provided error message. check_chat_prompt : Verifies if a Python code snippet is functioning as expected by examining the output. If the model decides that the output is inappropriate, it refines the code; otherwise, it returns the original code. streamlit_code_prompt : Generates Streamlit application code from a given goal, logic code, and test code. The model is tasked with creating Streamlit code that achieves the same objective as the test code.","title":"Prompt Templates"},{"location":"prompts/#understanding-prompt-templates","text":"Each prompt template is assembled from system and/or human message prompts. These prompts originate from template strings that characterize the model's role and define the task it needs to complete. The ChatPromptTemplate.from_messages() function compiles these message prompts into a comprehensive chat prompt that instructs the model to produce the required output. This method accepts an array of system and human message prompts, with the sequence they appear in the array determining the conversation flow with the model.","title":"Understanding Prompt Templates"},{"location":"prompts/#usage","text":"These prompt templates are utilized throughout the DemoGPT project to guide the GPT-3.5-turbo model in generating, testing, refining, and fixing Python code, as well as creating Streamlit applications. They form the basic building blocks of the project's interaction with the language model.","title":"Usage"}]}